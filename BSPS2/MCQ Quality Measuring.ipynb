{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60f43fb2-079e-4f46-bfea-f583046dce86",
   "metadata": {},
   "source": [
    "# MCQ Quality measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a645e1c-9069-4880-992c-0513494fc7f8",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a078cdd-7f7b-49a3-b642-5c0916bfc55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from python.questionType import *\n",
    "from python.quizMakerManager import *\n",
    "from python.quizMakerInteraction import *\n",
    "from python.quizGeneration import *\n",
    "from python.quizTaker import *\n",
    "from python.quizEditor  import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211f617e-7de0-4236-9e1d-d6e928c8abaa",
   "metadata": {},
   "source": [
    "## Test Suite MCQs generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012687ec-f6b1-48e5-a0d5-b17bc293195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-4o\" # \"gpt-4o\" or \"gpt-3.5-turbo-1106\"\n",
    "quizManagers = []\n",
    "for i in range(len(testSuite)):\n",
    "    # Quiz Creation\n",
    "    quizManagers.append(QuizFromTestSuite(testSuite[i]))\n",
    "\n",
    "    # Quiz Generation\n",
    "    fileNameToWrite=f\"testMCQ_{i}.json\"\n",
    "    promptBuild = promptBuilder(quizManagers[i])\n",
    "    generateQuiz(promptBuild, fileNameToWrite, model)\n",
    "    print(\"done\", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075f00da-a9a7-469b-94c6-939c6a170f56",
   "metadata": {},
   "source": [
    "## Test Suite MCQs quality measuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4839f95c-6794-4498-8363-34ff7cef2c41",
   "metadata": {},
   "outputs": [],
   "source": [
    " model = \"gpt-4o\" # \"gpt-4o\" or \"gpt-3.5-turbo-1106\"\n",
    "qualityMeasures = []\n",
    "for i in range(len(testSuite)):\n",
    "    fileNameToWrite=f\"testMCQ_{i}.json\"\n",
    "    quizData = loadQuiz(fileNameToWrite)  # Load your initial quiz data\n",
    "    qualityMeasures.append({})\n",
    "    print(evaluateQuiz(quizManagers[i], quizData, model, qualityMeasures[i]))\n",
    "    print(\"done\", i)\n",
    "    \n",
    "with open(\"qualityMeasures.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(qualityMeasures, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e395f5d-a091-4e12-80f7-b82a5004d4a5",
   "metadata": {},
   "source": [
    "## Calculating the average quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005958dc-01c2-41bf-94e9-79e56ffd99bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = 0\n",
    "i = 0\n",
    "for q in qualityMeasures:\n",
    "    if \"total\" in q:\n",
    "        avg += q[\"total\"]\n",
    "        i+=1\n",
    "print(avg/i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
